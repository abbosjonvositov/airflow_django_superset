{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe65642-4ada-4085-b64e-2425962ac19a",
   "metadata": {},
   "source": [
    "<h1>DATA LOADING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e79cf139-0d7e-481e-b4d4-fa5c1b5188e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "b36ca702-4e29-4887-8c27-a861eada2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('apartment_export_20250422_083809.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "d3b3aaad-a018-4a4e-ad6c-beaa75103ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = [\n",
    "    'Чиланзарский район', 'Юнусабадский район', 'Янгихаётский район',\n",
    "    'Яккасарайский район', 'Шайхантахурский район',\n",
    "    'Мирабадский район', 'Учтепинский район', 'Яшнабадский район',\n",
    "    'Бектемирский район', 'Сергелийский район',\n",
    "    'Мирзо-Улугбекский район', 'Алмазарский район', 'Новый Ташкентский район'\n",
    "]\n",
    "\n",
    "# Filter the DataFrame where district_name matches any of the districts\n",
    "tashkent_df = df[df['district_name'].isin(districts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "c0ff4a5a-5fbe-4e91-be44-21b6b30a0b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p1/5vh1vyrn3c91hc6lp5yz185m0000gn/T/ipykernel_46515/1874548450.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 43.  63.  nan ... 103.  67.  nan]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  tashkent_df.loc[:, columns_to_check] = tashkent_df[columns_to_check].apply(lambda x: np.where(x <= 0, np.nan, x))\n",
      "/var/folders/p1/5vh1vyrn3c91hc6lp5yz185m0000gn/T/ipykernel_46515/1874548450.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 1.  3.  3. ...  3.  2. nan]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  tashkent_df.loc[:, columns_to_check] = tashkent_df[columns_to_check].apply(lambda x: np.where(x <= 0, np.nan, x))\n",
      "/var/folders/p1/5vh1vyrn3c91hc6lp5yz185m0000gn/T/ipykernel_46515/1874548450.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 4.  2.  6. ...  7.  4. nan]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  tashkent_df.loc[:, columns_to_check] = tashkent_df[columns_to_check].apply(lambda x: np.where(x <= 0, np.nan, x))\n",
      "/var/folders/p1/5vh1vyrn3c91hc6lp5yz185m0000gn/T/ipykernel_46515/1874548450.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 5.  4. 16. ... 13.  8. nan]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  tashkent_df.loc[:, columns_to_check] = tashkent_df[columns_to_check].apply(lambda x: np.where(x <= 0, np.nan, x))\n",
      "/var/folders/p1/5vh1vyrn3c91hc6lp5yz185m0000gn/T/ipykernel_46515/1874548450.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1.8797441e+07 1.4829776e+07 8.6621040e+06 ... 2.0380000e+03           nan\n",
      "           nan]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  tashkent_df.loc[:, columns_to_check] = tashkent_df[columns_to_check].apply(lambda x: np.where(x <= 0, np.nan, x))\n",
      "/var/folders/p1/5vh1vyrn3c91hc6lp5yz185m0000gn/T/ipykernel_46515/1874548450.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[8.082900e+08 9.342759e+08 7.492720e+08 ... 2.100000e+05          nan\n",
      " 4.000000e+05]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  tashkent_df.loc[:, columns_to_check] = tashkent_df[columns_to_check].apply(lambda x: np.where(x <= 0, np.nan, x))\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Unknown' with NaN in specific columns using .loc[]\n",
    "columns_to_replace_unknown = ['foundation_name', 'layout_name', 'repair_name', 'wc_name']\n",
    "tashkent_df.loc[:, columns_to_replace_unknown] = tashkent_df[columns_to_replace_unknown].replace('Unknown', np.nan)\n",
    "\n",
    "# Replace values <= 0 with NaN in specific columns using .loc[]\n",
    "columns_to_check = ['total_area', 'number_of_rooms', 'floors', 'total_floors', \n",
    "                    'price_per_sqm', 'price', 'usd_uzs_rate']\n",
    "tashkent_df.loc[:, columns_to_check] = tashkent_df[columns_to_check].apply(lambda x: np.where(x <= 0, np.nan, x))\n",
    "tashkent_df = tashkent_df.drop(['city_name', 'region_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "2745df5b-3921-43eb-8b92-f1b0d7c21c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tashkent_df = tashkent_df.dropna(subset=['price', 'floors', 'number_of_rooms', 'total_area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "b282e71c-d983-41ae-ba92-f6a09dc69e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to fill\n",
    "columns_to_fill = ['foundation_name', 'layout_name', 'repair_name', 'wc_name']\n",
    "\n",
    "# Function to fill NaN values with the mode for each district, or global mode if district mode is not available\n",
    "def fill_missing_with_mode(df, columns, group_by='district_name'):\n",
    "    for column in columns:\n",
    "        # Get mode for each district\n",
    "        mode_values = df.groupby(group_by)[column].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "        \n",
    "        # Get the global mode for the entire dataset in case some districts have no mode\n",
    "        global_mode = df[column].mode()[0] if not df[column].mode().empty else None\n",
    "        \n",
    "        # Fill missing values for each district with the district mode, or fallback to global mode\n",
    "        df[column] = df.apply(\n",
    "            lambda row: mode_values[row[group_by]] if pd.isna(row[column]) and mode_values[row[group_by]] is not None \n",
    "                       else (global_mode if pd.isna(row[column]) else row[column]),\n",
    "            axis=1\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# Apply the function to fill missing values\n",
    "tashkent_df = fill_missing_with_mode(tashkent_df, columns_to_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "347f132b-e768-4921-afd0-98e7eebef5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessing_stage = tashkent_df.copy()\n",
    "df_preprocessing_stage['price_usd'] = df_preprocessing_stage['price'] / df_preprocessing_stage['usd_uzs_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "1fbf59cf-9c0e-4e99-b59a-f039bf8c213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of values to replace\n",
    "repair_name_mapping = {\n",
    "    'evro': 'Евроремонт',\n",
    "    'sredniy': 'Средний',\n",
    "    'chernovaya': 'Черновая отделка',\n",
    "    'custom':'Авторский проект',\n",
    "    'kapital':'Капитал'\n",
    "}\n",
    "\n",
    "# Apply the replacement to the 'repair_name' column\n",
    "df_preprocessing_stage['repair_name'] = df_preprocessing_stage['repair_name'].replace(repair_name_mapping)\n",
    "df_preprocessing_stage['year_month'] = df_preprocessing_stage['datetime'].dt.to_period('M')\n",
    "df_preprocessing_stage = df_preprocessing_stage.drop(['apartment_id', 'usd_uzs_rate', 'price', 'datetime', 'price_per_sqm'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "af506b06-d801-4649-b553-07f5e5f2dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessing_stage = df_preprocessing_stage[df_preprocessing_stage['floors'] <= df_preprocessing_stage['total_floors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "76f43172-9b16-401b-9d5d-d96a4ed19700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 209854 entries, 5 to 268931\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count   Dtype    \n",
      "---  ------           --------------   -----    \n",
      " 0   district_name    209854 non-null  object   \n",
      " 1   foundation_name  209854 non-null  object   \n",
      " 2   layout_name      209854 non-null  object   \n",
      " 3   repair_name      209854 non-null  object   \n",
      " 4   wc_name          209854 non-null  object   \n",
      " 5   total_area       209854 non-null  float64  \n",
      " 6   number_of_rooms  209854 non-null  float64  \n",
      " 7   floors           209854 non-null  float64  \n",
      " 8   total_floors     209854 non-null  float64  \n",
      " 9   price_usd        209854 non-null  float64  \n",
      " 10  year_month       209854 non-null  period[M]\n",
      "dtypes: float64(5), object(5), period[M](1)\n",
      "memory usage: 19.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_preprocessing_stage.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "ad3b83da-f9dc-4869-82bf-49dc9edb7668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Совмещенный', 'Раздельный', '2 санузла и более'], dtype=object)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessing_stage['wc_name'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a5f9ad-59f1-4b2c-9b71-20b21f26ee4e",
   "metadata": {},
   "source": [
    "<h1>EDA</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ac3607-6771-41ba-8e8a-9ccde876b7b2",
   "metadata": {},
   "source": [
    "<h2>HISTOGRAMS</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee00296-9ffd-458b-899a-81d091859048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the columns to plot\n",
    "columns_to_check = ['total_area', 'number_of_rooms', 'floors', 'total_floors', 'price_usd']\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(columns_to_check), figsize=(20, 10))\n",
    "\n",
    "# Plot each column\n",
    "for i, column in enumerate(columns_to_check):\n",
    "    axes[i].hist(df_preprocessing_stage[column], bins=20, color='blue', alpha=0.7)\n",
    "    axes[i].set_title(column)\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a006f-8740-4a2c-8bed-aada92df1d22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h2>CORRELATION MATRIX</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283d0c4-fd11-4406-8a6d-7d6835ee3f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Specify the columns to check\n",
    "columns_to_check = ['total_area', 'number_of_rooms', 'floors', 'total_floors', 'price_usd']\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df_preprocessing_stage[columns_to_check].corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae4af88-1931-4cc6-a64a-5bf0cf5d0139",
   "metadata": {},
   "source": [
    "<h1>OUTLIER ELEMINATION PIPELINE</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cb3f97-4893-49e7-a9f2-38a0cca06637",
   "metadata": {},
   "source": [
    "<h2>FILTRATION DEPARTING FROM BUSINESS CASE PERSPECTIVE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "72ddc4ab-6b21-4d4c-a163-df84aa490d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_area</th>\n",
       "      <th>number_of_rooms</th>\n",
       "      <th>floors</th>\n",
       "      <th>total_floors</th>\n",
       "      <th>price_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>202917.000000</td>\n",
       "      <td>202917.000000</td>\n",
       "      <td>202917.000000</td>\n",
       "      <td>202917.000000</td>\n",
       "      <td>2.029170e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>87.144823</td>\n",
       "      <td>2.484168</td>\n",
       "      <td>4.374907</td>\n",
       "      <td>7.104767</td>\n",
       "      <td>1.001733e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1038.540146</td>\n",
       "      <td>0.997988</td>\n",
       "      <td>2.690679</td>\n",
       "      <td>3.249067</td>\n",
       "      <td>1.372158e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.388539e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.809829e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.777234e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>81.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.149679e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>3.999744e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          total_area  number_of_rooms         floors   total_floors  \\\n",
       "count  202917.000000    202917.000000  202917.000000  202917.000000   \n",
       "mean       87.144823         2.484168       4.374907       7.104767   \n",
       "std      1038.540146         0.997988       2.690679       3.249067   \n",
       "min         1.000000         1.000000       1.000000       1.000000   \n",
       "25%        49.000000         2.000000       2.000000       4.000000   \n",
       "50%        63.000000         2.000000       4.000000       7.000000   \n",
       "75%        81.000000         3.000000       6.000000       9.000000   \n",
       "max    100000.000000         7.000000      16.000000      16.000000   \n",
       "\n",
       "          price_usd  \n",
       "count  2.029170e+05  \n",
       "mean   1.001733e+05  \n",
       "std    1.372158e+05  \n",
       "min    1.388539e-02  \n",
       "25%    5.809829e+04  \n",
       "50%    7.777234e+04  \n",
       "75%    1.149679e+05  \n",
       "max    3.999744e+07  "
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessing_stage = df_preprocessing_stage[\n",
    "    (df_preprocessing_stage['number_of_rooms'] <= 7) &\n",
    "    (df_preprocessing_stage['total_floors'] <= 16)\n",
    "]\n",
    "df_preprocessing_stage.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e9771-f3f5-477b-a1de-4e4ff66c546c",
   "metadata": {},
   "source": [
    "<h2>FUNCTION DEFINED</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "543a9219-fdaa-41a4-8619-7169421b7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import zscore\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product, permutations\n",
    "\n",
    "\n",
    "# Outlier elimination methods\n",
    "def eliminate_outliers_zscore(df, column, threshold=3):\n",
    "    return df[np.abs(zscore(df[column])) < threshold]\n",
    "\n",
    "\n",
    "def eliminate_outliers_iqr(df, column):\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "\n",
    "def eliminate_outliers_isolation_forest(df, column, contamination=0.01):\n",
    "    isolation_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "    df['Outlier_Flag'] = isolation_forest.fit_predict(df[[column]])\n",
    "    df = df[df['Outlier_Flag'] == 1]\n",
    "    return df.drop(columns=['Outlier_Flag'])\n",
    "\n",
    "\n",
    "def eliminate_outliers_quantile_clipping(df, column, lower_quantile=0.01, upper_quantile=0.99):\n",
    "    lower_bound = df[column].quantile(lower_quantile)\n",
    "    upper_bound = df[column].quantile(upper_quantile)\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "\n",
    "def eliminate_outliers_mad(df, column, threshold=3):\n",
    "    median = df[column].median()\n",
    "    mad = np.median(np.abs(df[column] - median))\n",
    "    df['MAD_Score'] = np.abs(df[column] - median) / (mad + 1e-9)\n",
    "    df = df[df['MAD_Score'] < threshold]\n",
    "    return df.drop(columns=['MAD_Score'])\n",
    "\n",
    "\n",
    "# One-hot encoding for categorical columns\n",
    "def apply_one_hot_encoding(df, categorical_columns):\n",
    "    df_encoded = pd.get_dummies(df, columns=categorical_columns)\n",
    "    for col in df_encoded.select_dtypes(include=['bool']).columns:\n",
    "        df_encoded[col] = df_encoded[col].astype(int)\n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "# Linear regression function\n",
    "def linear_regression_analysis(df, y_axis, dataset_title):\n",
    "    X = df.drop(columns=[y_axis])\n",
    "    y = df[y_axis]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return {'RMSE': rmse, 'MAE': mae, 'MSE': mse, 'MAPE': mape, 'R2': r2}\n",
    "\n",
    "\n",
    "def evaluate_all_combinations(df, target_column, categorical_columns):\n",
    "    # Columns to clean\n",
    "    columns_to_clean = ['total_area', 'price_usd']\n",
    "    \n",
    "    # Outlier elimination methods\n",
    "    outlier_methods = {\n",
    "        'Z-Score': eliminate_outliers_zscore,\n",
    "        'IQR': eliminate_outliers_iqr,\n",
    "        'Isolation Forest': eliminate_outliers_isolation_forest,\n",
    "        'Quantile Clipping': eliminate_outliers_quantile_clipping,\n",
    "        'MAD': eliminate_outliers_mad\n",
    "    }\n",
    "    \n",
    "    # Generate all possible combinations of methods across columns\n",
    "    method_combinations = list(product(outlier_methods.items(), repeat=len(columns_to_clean)))\n",
    "\n",
    "    # Include cases where a single method is applied to multiple columns\n",
    "    extended_combinations = []\n",
    "    for method_name, method in outlier_methods.items():\n",
    "        extended_combinations.append([(method_name, method)] * len(columns_to_clean))\n",
    "    \n",
    "    # Combine regular and extended combinations\n",
    "    all_combinations = method_combinations + extended_combinations\n",
    "    \n",
    "    results = []  # To store metrics for all combinations\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate through all combinations of methods\n",
    "    for method_sequence in all_combinations:\n",
    "        # Start with the original DataFrame\n",
    "        processed_df = df.copy()\n",
    "        columns_to_max = ['total_area', 'number_of_rooms', 'total_floors']\n",
    "        \n",
    "        # Apply each method in the sequence to the corresponding column\n",
    "        column_max_values = {}  # To store max values of columns\n",
    "        for column, (method_name, method) in zip(columns_to_clean, method_sequence):\n",
    "            try:\n",
    "                processed_df = method(processed_df, column)\n",
    "                column_max_values[column] = processed_df[column].max()  # Calculate max value after processing\n",
    "            except Exception as e:\n",
    "                print(f\"Error applying method {method_name} to column {column}: {e}\")\n",
    "                column_max_values[column] = None  # Handle errors gracefully\n",
    "        \n",
    "        # Calculate the row count after outlier elimination\n",
    "        row_count_after_elimination = len(processed_df)\n",
    "        \n",
    "        # Apply one-hot encoding for categorical columns\n",
    "        processed_df = apply_one_hot_encoding(processed_df, categorical_columns)\n",
    "        \n",
    "        # Calculate linear regression metrics\n",
    "        metrics = linear_regression_analysis(processed_df, target_column, \"Combination Pipeline\")\n",
    "        metrics['Pipeline_Sequence'] = \" -> \".join([f\"{method_name}({column})\" for column, (method_name, _) in zip(columns_to_clean, method_sequence)])\n",
    "        \n",
    "        # Add max values for columns to metrics\n",
    "        for column, max_value in column_max_values.items():\n",
    "            metrics[f\"Max_{column}\"] = max_value\n",
    "        \n",
    "        # Add the row count metric to metrics\n",
    "        metrics['Row_Count_After_Outlier_Elimination'] = row_count_after_elimination\n",
    "        \n",
    "        # Store metrics for this combination\n",
    "        results.append(metrics)\n",
    "        count += 1\n",
    "        print(f\"SEQ: {count}/{len(all_combinations)}\")\n",
    "    \n",
    "    # Convert the results list into a DataFrame\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def evaluate_all_combinations_permutations(df, target_column, categorical_columns):\n",
    "    # Columns to clean\n",
    "    columns_to_clean = ['total_area', 'price_usd']\n",
    "    \n",
    "    # Outlier elimination methods\n",
    "    outlier_methods = {\n",
    "        'IQR': eliminate_outliers_iqr,\n",
    "        'Isolation Forest': eliminate_outliers_isolation_forest,\n",
    "        'Quantile Clipping': eliminate_outliers_quantile_clipping,\n",
    "        'MAD': eliminate_outliers_mad\n",
    "    }\n",
    "    \n",
    "    # Generate all permutations of column order\n",
    "    column_orders = list(permutations(columns_to_clean))\n",
    "    \n",
    "    # Generate all possible combinations of methods across columns\n",
    "    method_combinations = list(product(outlier_methods.items(), repeat=len(columns_to_clean)))\n",
    "    \n",
    "    results = []  # To store metrics for all combinations\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through each column order\n",
    "    for column_order in column_orders:\n",
    "        # Iterate through each combination of methods\n",
    "        for method_sequence in method_combinations:\n",
    "            # Start with the original DataFrame\n",
    "            processed_df = df.copy()\n",
    "            column_max_values = {}  # To store max values of columns\n",
    "            \n",
    "            # Apply each method in the sequence to the corresponding column in the column order\n",
    "            for column, (method_name, method) in zip(column_order, method_sequence):\n",
    "                try:\n",
    "                    processed_df = method(processed_df, column)\n",
    "                    column_max_values[column] = processed_df[column].max()  # Calculate max value after processing\n",
    "                except Exception as e:\n",
    "                    print(f\"Error applying method {method_name} to column {column}: {e}\")\n",
    "                    column_max_values[column] = None  # Handle errors gracefully\n",
    "            \n",
    "            # Calculate the row count after outlier elimination\n",
    "            row_count_after_elimination = len(processed_df)\n",
    "            \n",
    "            # Apply one-hot encoding for categorical columns\n",
    "            processed_df = apply_one_hot_encoding(processed_df, categorical_columns)\n",
    "            \n",
    "            # Calculate linear regression metrics\n",
    "            metrics = linear_regression_analysis(processed_df, target_column, \"Combination Pipeline\")\n",
    "            metrics['Pipeline_Sequence'] = \" -> \".join([f\"{method_name}({column})\" for column, (method_name, _) in zip(column_order, method_sequence)])\n",
    "            metrics['Column_Order'] = \" -> \".join(column_order)\n",
    "            \n",
    "            # Add max values for columns to metrics\n",
    "            for column, max_value in column_max_values.items():\n",
    "                metrics[f\"Max_{column}\"] = max_value\n",
    "            \n",
    "            # Add the row count metric to metrics\n",
    "            metrics['Row_Count_After_Outlier_Elimination'] = row_count_after_elimination\n",
    "            \n",
    "            # Store metrics for this combination\n",
    "            results.append(metrics)\n",
    "            count += 1\n",
    "            print(f\"SEQ: {count}/{len(column_orders) * len(method_combinations)}\")\n",
    "    \n",
    "    # Convert the results list into a DataFrame\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d63ed7-d2d6-4b61-b8e9-13cb1ce16202",
   "metadata": {},
   "source": [
    "<h2>FUNCTION EXECUTION</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "e7d10464-9b51-43f1-a002-f6e618200e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_analysis(processed_df, target_column, pipeline_name):\n",
    "    # Splitting data into features and target\n",
    "    X = processed_df.drop(columns=[target_column])\n",
    "    y = processed_df[target_column]\n",
    "    \n",
    "    # Apply Random Forest regression and calculate metrics\n",
    "    metrics = random_forest(X, y)\n",
    "    \n",
    "    # Add pipeline name to the metrics\n",
    "    metrics['Pipeline_Sequence'] = pipeline_name\n",
    "    return metrics\n",
    "\n",
    "def evaluate_all_combinations_permutations_rf(df, target_column, categorical_columns):\n",
    "    # Columns to clean\n",
    "    columns_to_clean = ['total_area', 'price_usd']\n",
    "    \n",
    "    # Outlier elimination methods\n",
    "    outlier_methods = {\n",
    "        'IQR': eliminate_outliers_iqr,\n",
    "        'Isolation Forest': eliminate_outliers_isolation_forest,\n",
    "        'Quantile Clipping': eliminate_outliers_quantile_clipping,\n",
    "        'MAD': eliminate_outliers_mad\n",
    "    }\n",
    "    \n",
    "    # Generate all permutations of column order\n",
    "    column_orders = list(permutations(columns_to_clean))\n",
    "    \n",
    "    # Generate all possible combinations of methods across columns\n",
    "    method_combinations = list(product(outlier_methods.items(), repeat=len(columns_to_clean)))\n",
    "    \n",
    "    results = []  # To store metrics for all combinations\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through each column order\n",
    "    for column_order in column_orders:\n",
    "        # Iterate through each combination of methods\n",
    "        for method_sequence in method_combinations:\n",
    "            # Start with the original DataFrame\n",
    "            processed_df = df.copy()\n",
    "            column_max_values = {}  # To store max values of columns\n",
    "            \n",
    "            # Apply each method in the sequence to the corresponding column in the column order\n",
    "            for column, (method_name, method) in zip(column_order, method_sequence):\n",
    "                try:\n",
    "                    processed_df = method(processed_df, column)\n",
    "                    column_max_values[column] = processed_df[column].max()  # Calculate max value after processing\n",
    "                except Exception as e:\n",
    "                    print(f\"Error applying method {method_name} to column {column}: {e}\")\n",
    "                    column_max_values[column] = None  # Handle errors gracefully\n",
    "            \n",
    "            # Calculate the row count after outlier elimination\n",
    "            row_count_after_elimination = len(processed_df)\n",
    "            \n",
    "            # Apply one-hot encoding for categorical columns\n",
    "            processed_df = apply_one_hot_encoding(processed_df, categorical_columns)\n",
    "            \n",
    "            # Calculate Random Forest metrics\n",
    "            metrics = random_forest_analysis(processed_df, target_column, \"Combination Pipeline\")\n",
    "            metrics['Pipeline_Sequence'] = \" -> \".join([f\"{method_name}({column})\" for column, (method_name, _) in zip(column_order, method_sequence)])\n",
    "            metrics['Column_Order'] = \" -> \".join(column_order)\n",
    "            \n",
    "            # Add max values for columns to metrics\n",
    "            for column, max_value in column_max_values.items():\n",
    "                metrics[f\"Max_{column}\"] = max_value\n",
    "            \n",
    "            # Add the row count metric to metrics\n",
    "            metrics['Row_Count_After_Outlier_Elimination'] = row_count_after_elimination\n",
    "            \n",
    "            # Store metrics for this combination\n",
    "            results.append(metrics)\n",
    "            count += 1\n",
    "            print(f\"SEQ: {count}/{len(column_orders) * len(method_combinations)}\")\n",
    "    \n",
    "    # Convert the results list into a DataFrame\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "c6b3abb5-705a-46f0-a6db-6e3ef6df838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 1/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 2/32\n",
      "SEQ: 3/32\n",
      "SEQ: 4/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 5/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 6/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 7/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 8/32\n",
      "SEQ: 9/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 10/32\n",
      "SEQ: 11/32\n",
      "SEQ: 12/32\n",
      "SEQ: 13/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 14/32\n",
      "SEQ: 15/32\n",
      "SEQ: 16/32\n",
      "SEQ: 17/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 18/32\n",
      "SEQ: 19/32\n",
      "SEQ: 20/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 21/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 22/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 23/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 24/32\n",
      "SEQ: 25/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 26/32\n",
      "SEQ: 27/32\n",
      "SEQ: 28/32\n",
      "SEQ: 29/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ: 30/32\n",
      "SEQ: 31/32\n",
      "SEQ: 32/32\n"
     ]
    }
   ],
   "source": [
    "categorical_columns_list = ['district_name', 'foundation_name', 'layout_name','repair_name', 'wc_name', 'year_month']\n",
    "result_df_6_rf = evaluate_all_combinations_permutations_rf(df=df_preprocessing_stage, target_column = 'price_usd',categorical_columns=categorical_columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "a0d2d342-fcb0-4311-943d-f09c7d3169f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_6_rf.to_excel('outputs_excel_rf.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad755110-741d-4001-879b-8d923b778d40",
   "metadata": {},
   "source": [
    "<h2>APPLY OUTLIER ELEMINATION</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb327380-97f4-41cc-8d98-7383b80f5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_selected_sequence(df, categorical_columns):\n",
    "    # Columns to clean and corresponding methods\n",
    "    cleaning_sequence = [\n",
    "        ('total_area', 'Quantile Clipping', eliminate_outliers_quantile_clipping),\n",
    "        ('price_usd', 'Quantile Clipping', eliminate_outliers_quantile_clipping)\n",
    "    ]\n",
    "\n",
    "    # Start with the original DataFrame\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # Apply the selected methods in sequence to the corresponding columns\n",
    "    for column, method_name, method in cleaning_sequence:\n",
    "        try:\n",
    "            processed_df = method(processed_df, column)\n",
    "        except Exception as e:\n",
    "            print(f\"Error applying method {method_name} to column {column}: {e}\")\n",
    "            continue  # Skip this column if there's an error\n",
    "\n",
    "    # Apply one-hot encoding for categorical columns\n",
    "    processed_df = apply_one_hot_encoding(processed_df, categorical_columns)\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "categorical_columns_list = ['district_name', 'foundation_name', 'layout_name','repair_name', 'wc_name', 'year_month']\n",
    "cleaned_df = evaluate_selected_sequence(df_preprocessing_stage, categorical_columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e2c21-72a5-4259-a054-5456b2110d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410579c5-7980-4795-bf14-373e31ea4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the columns to plot\n",
    "columns_to_check = ['total_area', 'number_of_rooms', 'floors', 'total_floors', 'price_usd']\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(columns_to_check), figsize=(20, 10))\n",
    "\n",
    "# Plot each column\n",
    "for i, column in enumerate(columns_to_check):\n",
    "    axes[i].hist(cleaned_df[column], bins=30, color='blue', alpha=0.7)\n",
    "    axes[i].set_title(column)\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a20af14-97b4-4273-b775-50ba0da18c37",
   "metadata": {},
   "source": [
    "<h1>DATA PREPARATION</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15aa821-f0ff-46fc-a4e5-3d9d8f78a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_selected_sequence(df, categorical_columns):\n",
    "    # Columns to clean and corresponding methods\n",
    "    cleaning_sequence = [\n",
    "        ('total_area', 'Quantile Clipping', eliminate_outliers_quantile_clipping),\n",
    "        ('number_of_rooms', 'Z-Score', eliminate_outliers_zscore),\n",
    "        ('total_floors', 'IQR', eliminate_outliers_iqr),\n",
    "        ('price_usd', 'MAD', eliminate_outliers_mad)\n",
    "    ]\n",
    "\n",
    "    # Start with the original DataFrame\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # Apply the selected methods in sequence to the corresponding columns\n",
    "    for column, method_name, method in cleaning_sequence:\n",
    "        try:\n",
    "            processed_df = method(processed_df, column)\n",
    "        except Exception as e:\n",
    "            print(f\"Error applying method {method_name} to column {column}: {e}\")\n",
    "            continue  # Skip this column if there's an error\n",
    "\n",
    "    # Apply one-hot encoding for categorical columns\n",
    "    processed_df = apply_one_hot_encoding(processed_df, categorical_columns)\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "categorical_columns_list = ['district_name', 'foundation_name', 'layout_name','repair_name', 'wc_name']\n",
    "cleaned_df = evaluate_selected_sequence(df_preprocessing_stage, categorical_columns_list)\n",
    "X = cleaned_df.drop(columns=['price_usd'])\n",
    "y = cleaned_df['price_usd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a49a00-6729-4c85-8356-b6603cbff330",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_excel('data_for_rf.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb5018-a43a-4d52-ad06-f05c754cd152",
   "metadata": {},
   "source": [
    "<h1>HYPERPARAMETER TUNING</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd72c1b-be5c-4643-a1a3-dcba924d7ef5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h2>FUNCTIONS DEFINED</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a413a62-1c5d-494f-8599-f72c295368e8",
   "metadata": {},
   "source": [
    "<h3>GRID SEARCH</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993e1cf-043c-414f-893c-ecdf524bbdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import VotingRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "def preprocess_data(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def calculate_metrics(y_test, y_pred):\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return {\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"MAPE\": mape,\n",
    "        \"R2\": r2\n",
    "    }\n",
    "\n",
    "\n",
    "# Modified function with loss visualization\n",
    "def optimized_model_with_kfold_and_loss(model, X, y, param_grid=None, model_name=\"Model\"):\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(X, y)\n",
    "\n",
    "    # K-Fold Cross Validation\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)  # 5 folds\n",
    "    if param_grid:\n",
    "        # Grid Search Optimization with K-Fold\n",
    "        grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=kfold, verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    else:\n",
    "        # No Grid Search, only K-Fold\n",
    "        best_model = model\n",
    "        for train_idx, val_idx in kfold.split(X_train):\n",
    "            best_model.fit(X_train[train_idx], y_train[train_idx])\n",
    "            val_pred = best_model.predict(X_train[val_idx])\n",
    "\n",
    "    # Fit on the entire training set with best model\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    return calculate_metrics(y_test, y_test_pred), best_model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48cfe42-b385-408d-9467-d62445915f89",
   "metadata": {},
   "source": [
    "<h3>Traditional ML</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4524d353-a89f-48ff-8920-7af02fc8045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_decision_tree(X, y):\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 10, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    model_name = \"DecisionTree\"\n",
    "    model = DecisionTreeRegressor(random_state=23)\n",
    "    metrics, best_model = optimized_model_with_kfold_and_loss(model, X, y, param_grid=param_grid, model_name=model_name)\n",
    "    return metrics, best_model\n",
    "\n",
    "def optimized_random_forest(X, y):\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 10, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    model_name = \"RandomForest\"\n",
    "    model = RandomForestRegressor(random_state=23)\n",
    "    metrics, best_model = optimized_model_with_kfold_and_loss(model, X, y, param_grid=param_grid, model_name=model_name)\n",
    "    return metrics, best_model\n",
    "\n",
    "def optimized_knn(X, y):\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 10],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "    model_name = \"KNN\"\n",
    "    model = KNeighborsRegressor()\n",
    "    metrics, best_model = optimized_model_with_kfold_and_loss(model, X, y, param_grid=param_grid, model_name=model_name)\n",
    "    return metrics, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604fad27-8eaf-46b4-a8e4-ae5d3137fa24",
   "metadata": {},
   "source": [
    "<h3>Ensemble Methods</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a547467-2803-4181-9043-211e9bbfcf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_xgboost(X, y):\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "    model_name = \"XGBoost\"\n",
    "    model = XGBRegressor(random_state=23)\n",
    "    metrics, best_model = optimized_model_with_kfold_and_loss(model, X, y, param_grid=param_grid, model_name=model_name)\n",
    "    return metrics, best_model\n",
    "\n",
    "def optimized_catboost(X, y):\n",
    "    param_grid = {\n",
    "        'iterations': [100, 200],\n",
    "        'depth': [3, 5, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "    model_name = \"CatBoost\"\n",
    "    model = CatBoostRegressor(silent=True, random_state=23)\n",
    "    metrics, best_model = optimized_model_with_kfold_and_loss(model, X, y, param_grid=param_grid, model_name=model_name)\n",
    "    return metrics, best_model\n",
    "\n",
    "def optimized_lightgbm(X, y):\n",
    "    from lightgbm import LGBMRegressor\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'num_leaves': [20, 31, 50]\n",
    "    }\n",
    "    model_name = \"LightGBM\"\n",
    "    model = LGBMRegressor(random_state=23)\n",
    "    metrics, best_model = optimized_model_with_kfold_and_loss(model, X, y, param_grid=param_grid, model_name=model_name)\n",
    "    return metrics, best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbc9b9-46f3-4a66-80d7-e3ac9f87ef86",
   "metadata": {},
   "source": [
    "<h3>ANN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f427e48-f3a5-4916-9aff-dc92441a0f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_mlp(X, y):\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    }\n",
    "    model_name = \"MLP\"\n",
    "    model = MLPRegressor(random_state=23, max_iter=500)\n",
    "    metrics, best_model = optimized_model_with_kfold_and_loss(model, X, y, param_grid=param_grid, model_name=model_name)\n",
    "    return metrics, best_model\n",
    "\n",
    "def optimized_lstm(X, y, timesteps):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    import numpy as np\n",
    "\n",
    "    # Reshaping data for LSTM model\n",
    "    X_reshaped = np.array([X[i:i+timesteps] for i in range(len(X) - timesteps)])\n",
    "    y_reshaped = y[timesteps:]\n",
    "\n",
    "    def build_model(units, learning_rate):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units, input_shape=(timesteps, X.shape[1])))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mse', learning_rate=learning_rate)\n",
    "        return model\n",
    "\n",
    "    param_grid = {\n",
    "        'units': [50, 100],\n",
    "        'learning_rate': [0.01, 0.001]\n",
    "    }\n",
    "    model_name = \"LSTM\"\n",
    "    # Implementing a wrapper or using GridSearchCV with Keras models\n",
    "    metrics, best_model = optimized_model_with_kfold_and_loss(build_model, X_reshaped, y_reshaped, param_grid=param_grid, model_name=model_name)\n",
    "    return metrics, best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff862b-31c2-437b-b606-f27222f4fce6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h2>FUNCTIONS EXECUTED</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daaec4d-ecf1-4f59-9f93-8e29d4aab4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_metrics, dt_best_model = optimized_decision_tree(X, y)\n",
    "# random_forrest_metircs, rf_best_model = optimized_random_forest(X, y)\n",
    "# knn_metrics, knn_best_model = optimized_knn(X, y)\n",
    "# xgboost_metrics, xgboost_best_model = optimized_xgboost(X, y)\n",
    "# catboost_metrics, catboost_model = optimized_catboost(X, y)\n",
    "\n",
    "\n",
    "models_measurement_optimized = {}\n",
    "\n",
    "#Traditional ML\n",
    "models_measurement_optimized['Decision Tree'] = decision_tree_metrics\n",
    "# models_measurement['Random Forrest'] = random_forrest_metircs\n",
    "# models_measurement['KNN'] = knn_metrics\n",
    "# models_measurement['XGBoost'] = xgboost_metrics\n",
    "# models_measurement['CatBoost'] = catboost_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c75b0-d434-4b01-b7e7-d913d1d1726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_metrics, lstm_best_model = optimized_mlp(X, y)(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a52b03-924a-4b97-b1f3-e630ba5ad617",
   "metadata": {},
   "source": [
    "<h1>TRAIN & TEST SET LOSS FUNCTION</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae88a4f-65b7-4bfd-9253-26fdf2ecf72e",
   "metadata": {},
   "source": [
    "<h2>RANDOM FORREST</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf17123-9b56-41d8-b5a9-41ed141a01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def random_forest_with_loss(X, y):\n",
    "    # Preprocess data\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(X, y)\n",
    "    \n",
    "    # Store MSE values\n",
    "    train_mse_values = []\n",
    "    test_mse_values = []\n",
    "    n_trees = range(1, 101)\n",
    "\n",
    "    for i, n in enumerate(n_trees):\n",
    "        # Initialize model\n",
    "        model = RandomForestRegressor(n_estimators=n, random_state=42)\n",
    "        \n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate MSE for train and test\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        \n",
    "        train_mse_values.append(train_mse)\n",
    "        test_mse_values.append(test_mse)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Completed {i + 1}/{len(n_trees)} iterations. {len(n_trees) - (i + 1)} left.\")\n",
    "\n",
    "    # Plot MSE values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(n_trees, train_mse_values, label=\"Train MSE\", marker='o')\n",
    "    plt.plot(n_trees, test_mse_values, label=\"Test MSE\", marker='o')\n",
    "    plt.title(\"Loss (MSE) vs Number of Trees\")\n",
    "    plt.xlabel(\"Number of Trees\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "random_forest_with_loss(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda1cad-ff82-4d16-aa53-39026d027cfd",
   "metadata": {},
   "source": [
    "<h2>CAT BOOST</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a09841-eca6-443b-8157-aa0cdb490080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "def catboost_with_plot(X, y):\n",
    "    # Preprocessing data\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(X, y)\n",
    "    \n",
    "    # Training the model\n",
    "    model = CatBoostRegressor(silent=True)\n",
    "    model.fit(X_train, y_train, eval_set=(X_test, y_test), plot=True)\n",
    "    \n",
    "    # Extracting metrics for visualization\n",
    "    evals_result = model.get_evals_result()\n",
    "    \n",
    "    # Plotting loss function\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(evals_result['learn']['RMSE'], label=\"Train\", color='blue')\n",
    "    plt.plot(evals_result['validation']['RMSE'], label=\"Test\", color='orange')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss (RMSE)\")\n",
    "    plt.title(\"Loss Function Plot\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # Making predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    return calculate_metrics(y_test, y_pred)\n",
    "\n",
    "catboost_with_plot(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c325d-b525-4d94-a0d0-6d6b8a3830b2",
   "metadata": {},
   "source": [
    "<h2>XGBOOST</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d77d13-6494-49c3-987f-f05e7c990a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def xgboost(X, y):\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(X, y)\n",
    "\n",
    "    # Initialize the model with early stopping options to capture loss\n",
    "    model = XGBRegressor(eval_metric=\"rmse\")\n",
    "    \n",
    "    # Fit the model while tracking training and validation losses\n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    model.fit(X_train, y_train, eval_set=eval_set, verbose=True)\n",
    "\n",
    "    # Retrieve the loss values from the training process\n",
    "    results = model.evals_result()\n",
    "    train_loss = results[\"validation_0\"][\"rmse\"]\n",
    "    test_loss = results[\"validation_1\"][\"rmse\"]\n",
    "\n",
    "    # Plot the training and test loss functions\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(np.arange(len(train_loss)), train_loss, label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(np.arange(len(test_loss)), test_loss, label=\"Test Loss\", marker=\"x\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"RMSE Loss\")\n",
    "    plt.title(\"XGBoost Training vs Test Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    return calculate_metrics(y_test, y_pred)\n",
    "\n",
    "\n",
    "xgboost(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec00de3-4e90-4cbd-9a9e-1de6074c2a01",
   "metadata": {},
   "source": [
    "<h2>DECISION TREE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3746975-f2f4-40ea-a6fe-84530d4e86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def decision_tree(X, y):\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(X, y)\n",
    "\n",
    "    depths = range(1, 21)  # Test different tree depths\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for depth in depths:\n",
    "        model = DecisionTreeRegressor(max_depth=depth)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        train_loss = mean_squared_error(y_train, y_train_pred)\n",
    "        test_loss = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "    # Plot training and test loss curves\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(depths, train_losses, label=\"Train Loss (MSE)\", marker=\"o\", linestyle=\"-\")\n",
    "    plt.plot(depths, test_losses, label=\"Test Loss (MSE)\", marker=\"x\", linestyle=\"-\")\n",
    "    plt.xlabel(\"Tree Depth\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"Decision Tree Training vs Test Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Final model selection at optimal depth\n",
    "\n",
    "decision_tree(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad86106-eed3-4b28-a203-eaeb5208aea8",
   "metadata": {},
   "source": [
    "<h2>KNN</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b6b590-1740-4165-93e9-75a91b2109ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def knn(X, y):\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(X, y)\n",
    "\n",
    "    k_values = range(1, 21)  # Testing different values of k\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for k in k_values:\n",
    "        model = KNeighborsRegressor(n_neighbors=k)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        train_loss = mean_squared_error(y_train, y_train_pred)\n",
    "        test_loss = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "    # Plot training and test loss curves\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(k_values, train_losses, label=\"Train Loss (MSE)\", marker=\"o\", linestyle=\"-\")\n",
    "    plt.plot(k_values, test_losses, label=\"Test Loss (MSE)\", marker=\"x\", linestyle=\"-\")\n",
    "    plt.xlabel(\"Number of Neighbors (k)\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"KNN Training vs Test Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Final model selection at optimal k\n",
    "    best_k = k_values[np.argmin(test_losses)]\n",
    "    final_model = KNeighborsRegressor(n_neighbors=best_k)\n",
    "    final_model.fit(X_train, y_train)\n",
    "    y_pred = final_model.predict(X_test)\n",
    "\n",
    "    return calculate_metrics(y_test, y_pred)\n",
    "\n",
    "\n",
    "knn(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
